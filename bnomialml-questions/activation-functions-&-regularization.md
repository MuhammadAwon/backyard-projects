# Odd one out

Your goal is to determine which of the following doesn't belong in this list.

**Can you select the odd one out?**

- [x] ReLU
- [x] Sigmoid
- [ ] Dropout
- [x] Softmax

## There you go!

[ReLU](https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/), [Sigmoid](https://machinelearningmastery.com/a-gentle-introduction-to-sigmoid-function/), and [Softmax](https://machinelearningmastery.com/softmax-activation-function-with-python/) are all activation functions used to train neural networks. They all take in the weighted sum of all of the inputs from the previous layer and generate an output value to the next layer. [Dropout](https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/) is a regularization method also used to train neural networks.

The third choice is the correct answer.

## References

* [A Gentle Introduction to Dropout for Regularizing Deep Neural Networks](https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/)
* [A Gentle Introduction to the Rectified Linear Unit (ReLU)](https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/)
* [A Gentle Introduction To Sigmoid Function](https://machinelearningmastery.com/a-gentle-introduction-to-sigmoid-function/)
* [Softmax Activation Function with Python](https://machinelearningmastery.com/softmax-activation-function-with-python/)