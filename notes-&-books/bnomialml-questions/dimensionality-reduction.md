# Cutting down features

When Nicole finished collecting the data, she realized that there were just too many features.

She was staring at hundreds of potential variables, and it was evident that any model would have a hard time navigating them. Nicole knew that she had to reduce the dimensionality of her dataset.

Dimensionality reduction algorithms reduce the number of input variables in a dataset to find a lower-dimensional representation that still preserves the salient relationships in the data.

**Which of the following are dimensionality reduction techniques that Nicole could use?**

- [x] Singular Value Decomposition
- [x] Principal Component Analysis
- [x] Linear Discriminant Analysis
- [x] Isomap Embedding

## Explanation

Every one of these is a valid dimensionality reduction technique.

The problem doesn't specify the type of data that Nicole is using, so it's hard to determine which of these techniques will be most effective, but every one of them could be potentially valuable.

Therefore, all choices are correct.

## Recommendations

- [Singular value decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition)
- [Understanding Dimension Reduction with Principal Component Analysis (PCA)](https://blog.paperspace.com/dimension-reduction-with-principal-component-analysis/)
- [Linear Discriminant Analysis â€“ Bit by Bit](https://sebastianraschka.com/Articles/2014_python_lda.html)
- [Dimension Reduction - IsoMap](https://blog.paperspace.com/dimension-reduction-with-isomap/)
